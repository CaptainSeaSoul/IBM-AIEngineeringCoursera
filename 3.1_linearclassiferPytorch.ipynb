{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Objective</h1><ul><li> How to use linear classifier in pytorch.</li></ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear  Classifier with PyTorch </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before you use a  Deep neural network to solve the classification problem,  it 's a good idea to try and solve the problem with the simplest method. You will need the dataset object from the previous section.\n",
    "In this lab, we solve the problem with a linear classifier.\n",
    " You will be asked to determine the maximum accuracy your linear classifier can achieve on the validation data for 5 epochs. We will give some free parameter values if you follow the instructions you will be able to answer the quiz. Just like the other labs there are several steps, but in this lab you will only be quizzed on the final result. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#download_data\"> Download data</a></li>\n",
    "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
    "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
    "    <li><a href=\"#trasform_Data_object\">Transform Object and Dataset Object</a></li>\n",
    "    <li><a href=\"#Question\">Question</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    " </div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using <b>wget</b>, then unzip them.  <b>wget</b> is a command the retrieves content from web servers, in this case its a zip file. Locally we store the data in the directory  <b>/resources/data</b> . The <b>-p</b> creates the entire directory tree up to the given directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the file that contains the images, if you dint do this in your first lab uncomment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# !python -m wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip -o ./resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then unzip the file, this ma take a while:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -q  ./resources/data/concrete_crack_images_for_classification.zip -d  ./resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then download the files that contain the negative images:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch import optim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the previous code to build a dataset class. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter <code>train</code> is set to <code>True</code>, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data. Do not forget to sort your files so they are in the same order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\r\n",
    "\r\n",
    "    # Constructor\r\n",
    "    def __init__(self,transform=None,train=True):\r\n",
    "        directory=\"./resources/data\"\r\n",
    "        positive=\"Positive\"\r\n",
    "        negative=\"Negative\"\r\n",
    "\r\n",
    "        positive_file_path=os.path.join(directory,positive)\r\n",
    "        negative_file_path=os.path.join(directory,negative)\r\n",
    "        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\r\n",
    "        positive_files.sort()\r\n",
    "        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\r\n",
    "        negative_files.sort()\r\n",
    "        number_of_samples=len(positive_files)+len(negative_files)\r\n",
    "        self.all_files=[None]*number_of_samples\r\n",
    "        self.all_files[::2]=positive_files\r\n",
    "        self.all_files[1::2]=negative_files \r\n",
    "        # The transform is goint to be used on image\r\n",
    "        self.transform = transform\r\n",
    "        #torch.LongTensor\r\n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\r\n",
    "        self.Y[::2]=1\r\n",
    "        self.Y[1::2]=0\r\n",
    "        \r\n",
    "        if train:\r\n",
    "            self.all_files=self.all_files[0:30000]\r\n",
    "            self.Y=self.Y[0:30000]\r\n",
    "            self.len=len(self.all_files)\r\n",
    "        else:\r\n",
    "            self.all_files=self.all_files[30000:]\r\n",
    "            self.Y=self.Y[30000:]\r\n",
    "            self.len=len(self.all_files)    \r\n",
    "       \r\n",
    "    # Get the length\r\n",
    "    def __len__(self):\r\n",
    "        return self.len\r\n",
    "    \r\n",
    "    # Getter\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        \r\n",
    "        \r\n",
    "        image=Image.open(self.all_files[idx])\r\n",
    "        y=self.Y[idx]\r\n",
    "          \r\n",
    "        \r\n",
    "        # If there is any transform method, apply it onto the image\r\n",
    "        if self.transform:\r\n",
    "            image = self.transform(image)\r\n",
    "\r\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"trasform_Data_object\">Transform Object and Dataset Object</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transform object, that uses the <code>Compose</code> function. First use the transform <code>ToTensor()</code> and followed by <code>Normalize(mean, std)</code>. The value for <code> mean</code> and <code>std</code> are provided for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\r\n",
    "std = [0.229, 0.224, 0.225]\r\n",
    "# transforms.ToTensor()\r\n",
    "#transforms.Normalize(mean, std)\r\n",
    "#transforms.Compose([])\r\n",
    "\r\n",
    "transform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create object for the training data  <code>dataset_train</code> and validation <code>dataset_val</code>. Use the transform object to convert the images to tensors using the transform object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=Dataset(transform=transform,train=True)\r\n",
    "dataset_val=Dataset(transform=transform,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  can find the shape of the image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it's a color image with three channels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154587"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_image=3*227*227\r\n",
    "size_of_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question\"> Question <h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create a custom module for Softmax for two classes,called model. The input size should be the <code>size_of_image</code>, you should record the maximum accuracy achieved on the validation data for the different epochs. For example if the 5 epochs the accuracy was 0.5, 0.2, 0.64,0.77, 0.66 you would select 0.77.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with the following free parameter values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Parameter Values</b>\n",
    "\n",
    "   <li>learning rate:0.1 </li>\n",
    "   <li>momentum term:0.1 </li>\n",
    "   <li>batch size training:1000</li>\n",
    "   <li>Loss function:Cross Entropy Loss </li>\n",
    "   <li>epochs:5</li>\n",
    "   <li>set: torch.manual_seed(0)</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c6dbd0f210>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Custom Module:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSoftmax(torch.nn.Module):\r\n",
    "    def __init__(self, size_of_image, num_classes=2):\r\n",
    "        super(CustomSoftmax, self).__init__()\r\n",
    "        self.linear = nn.Linear(size_of_image, num_classes)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        return self.linear(x)\r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model Object:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomSoftmax(size_of_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optimizer:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Criterion:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Loader Training and Validation:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=1000)\r\n",
    "valid_data_loader = torch.utils.data.DataLoader(dataset_val, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train Model with 5 epochs, should take 35 minutes: </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "training:   3%|▎         | 1/30 [00:02<01:06,  2.30s/it]\u001b[A\n",
      "training:   7%|▋         | 2/30 [00:04<01:08,  2.44s/it]\u001b[A\n",
      "training:  10%|█         | 3/30 [00:07<01:07,  2.49s/it]\u001b[A\n",
      "training:  13%|█▎        | 4/30 [00:09<01:05,  2.50s/it]\u001b[A\n",
      "training:  17%|█▋        | 5/30 [00:12<01:01,  2.46s/it]\u001b[A\n",
      "training:  20%|██        | 6/30 [00:14<01:00,  2.53s/it]\u001b[A\n",
      "training:  23%|██▎       | 7/30 [00:17<00:57,  2.51s/it]\u001b[A\n",
      "training:  27%|██▋       | 8/30 [00:20<00:56,  2.59s/it]\u001b[A\n",
      "training:  30%|███       | 9/30 [00:23<00:57,  2.72s/it]\u001b[A\n",
      "training:  33%|███▎      | 10/30 [00:25<00:54,  2.72s/it]\u001b[A\n",
      "training:  37%|███▋      | 11/30 [00:28<00:49,  2.61s/it]\u001b[A\n",
      "training:  40%|████      | 12/30 [00:31<00:47,  2.65s/it]\u001b[A\n",
      "training:  43%|████▎     | 13/30 [00:33<00:43,  2.56s/it]\u001b[A\n",
      "training:  47%|████▋     | 14/30 [00:35<00:40,  2.53s/it]\u001b[A\n",
      "training:  50%|█████     | 15/30 [00:38<00:37,  2.48s/it]\u001b[A\n",
      "training:  53%|█████▎    | 16/30 [00:40<00:34,  2.44s/it]\u001b[A\n",
      "training:  57%|█████▋    | 17/30 [00:42<00:31,  2.42s/it]\u001b[A\n",
      "training:  60%|██████    | 18/30 [00:45<00:28,  2.39s/it]\u001b[A\n",
      "training:  63%|██████▎   | 19/30 [00:47<00:27,  2.50s/it]\u001b[A\n",
      "training:  67%|██████▋   | 20/30 [00:50<00:25,  2.53s/it]\u001b[A\n",
      "training:  70%|███████   | 21/30 [00:52<00:22,  2.47s/it]\u001b[A\n",
      "training:  73%|███████▎  | 22/30 [00:55<00:19,  2.45s/it]\u001b[A\n",
      "training:  77%|███████▋  | 23/30 [00:57<00:16,  2.43s/it]\u001b[A\n",
      "training:  80%|████████  | 24/30 [01:00<00:14,  2.41s/it]\u001b[A\n",
      "training:  83%|████████▎ | 25/30 [01:02<00:12,  2.43s/it]\u001b[A\n",
      "training:  87%|████████▋ | 26/30 [01:04<00:09,  2.43s/it]\u001b[A\n",
      "training:  90%|█████████ | 27/30 [01:07<00:07,  2.42s/it]\u001b[A\n",
      "training:  93%|█████████▎| 28/30 [01:09<00:04,  2.41s/it]\u001b[A\n",
      "training:  97%|█████████▋| 29/30 [01:12<00:02,  2.40s/it]\u001b[A\n",
      "training: 100%|██████████| 30/30 [01:14<00:00,  2.48s/it]\u001b[A\n",
      "\n",
      "validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "validating:  10%|█         | 1/10 [00:02<00:21,  2.34s/it]\u001b[A\n",
      "validating:  20%|██        | 2/10 [00:04<00:19,  2.46s/it]\u001b[A\n",
      "validating:  30%|███       | 3/10 [00:07<00:16,  2.40s/it]\u001b[A\n",
      "validating:  40%|████      | 4/10 [00:09<00:14,  2.38s/it]\u001b[A\n",
      "validating:  50%|█████     | 5/10 [00:11<00:11,  2.36s/it]\u001b[A\n",
      "validating:  60%|██████    | 6/10 [00:14<00:09,  2.33s/it]\u001b[A\n",
      "validating:  70%|███████   | 7/10 [00:16<00:06,  2.31s/it]\u001b[A\n",
      "validating:  80%|████████  | 8/10 [00:19<00:04,  2.39s/it]\u001b[A\n",
      "validating:  90%|█████████ | 9/10 [00:21<00:02,  2.42s/it]\u001b[A\n",
      "validating: 100%|██████████| 10/10 [00:23<00:00,  2.38s/it]\u001b[A\n",
      "epochs:  20%|██        | 1/5 [01:38<06:33, 98.26s/it]\n",
      "training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "training:   3%|▎         | 1/30 [00:02<01:10,  2.43s/it]\u001b[A\n",
      "training:   7%|▋         | 2/30 [00:04<01:06,  2.39s/it]\u001b[A\n",
      "training:  10%|█         | 3/30 [00:07<01:04,  2.39s/it]\u001b[A\n",
      "training:  13%|█▎        | 4/30 [00:09<01:01,  2.37s/it]\u001b[A\n",
      "training:  17%|█▋        | 5/30 [00:11<00:59,  2.40s/it]\u001b[A\n",
      "training:  20%|██        | 6/30 [00:14<00:57,  2.41s/it]\u001b[A\n",
      "training:  23%|██▎       | 7/30 [00:16<00:55,  2.40s/it]\u001b[A\n",
      "training:  27%|██▋       | 8/30 [00:19<00:52,  2.38s/it]\u001b[A\n",
      "training:  30%|███       | 9/30 [00:21<00:49,  2.37s/it]\u001b[A\n",
      "training:  33%|███▎      | 10/30 [00:23<00:47,  2.36s/it]\u001b[A\n",
      "training:  37%|███▋      | 11/30 [00:26<00:45,  2.37s/it]\u001b[A\n",
      "training:  40%|████      | 12/30 [00:28<00:43,  2.42s/it]\u001b[A\n",
      "training:  43%|████▎     | 13/30 [00:31<00:41,  2.41s/it]\u001b[A\n",
      "training:  47%|████▋     | 14/30 [00:33<00:38,  2.38s/it]\u001b[A\n",
      "training:  50%|█████     | 15/30 [00:36<00:36,  2.44s/it]\u001b[A\n",
      "training:  53%|█████▎    | 16/30 [00:38<00:34,  2.47s/it]\u001b[A\n",
      "training:  57%|█████▋    | 17/30 [00:40<00:31,  2.45s/it]\u001b[A\n",
      "training:  60%|██████    | 18/30 [00:43<00:29,  2.44s/it]\u001b[A\n",
      "training:  63%|██████▎   | 19/30 [00:46<00:27,  2.52s/it]\u001b[A\n",
      "training:  67%|██████▋   | 20/30 [00:48<00:24,  2.49s/it]\u001b[A\n",
      "training:  70%|███████   | 21/30 [00:50<00:22,  2.49s/it]\u001b[A\n",
      "training:  73%|███████▎  | 22/30 [00:53<00:20,  2.51s/it]\u001b[A\n",
      "training:  77%|███████▋  | 23/30 [00:55<00:17,  2.47s/it]\u001b[A\n",
      "training:  80%|████████  | 24/30 [00:58<00:15,  2.53s/it]\u001b[A\n",
      "training:  83%|████████▎ | 25/30 [01:01<00:12,  2.57s/it]\u001b[A\n",
      "training:  87%|████████▋ | 26/30 [01:03<00:09,  2.49s/it]\u001b[A\n",
      "training:  90%|█████████ | 27/30 [01:06<00:07,  2.50s/it]\u001b[A\n",
      "training:  93%|█████████▎| 28/30 [01:08<00:04,  2.46s/it]\u001b[A\n",
      "training:  97%|█████████▋| 29/30 [01:10<00:02,  2.43s/it]\u001b[A\n",
      "training: 100%|██████████| 30/30 [01:13<00:00,  2.45s/it]\u001b[A\n",
      "\n",
      "validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "validating:  10%|█         | 1/10 [00:02<00:23,  2.63s/it]\u001b[A\n",
      "validating:  20%|██        | 2/10 [00:04<00:19,  2.45s/it]\u001b[A\n",
      "validating:  30%|███       | 3/10 [00:07<00:16,  2.38s/it]\u001b[A\n",
      "validating:  40%|████      | 4/10 [00:09<00:14,  2.37s/it]\u001b[A\n",
      "validating:  50%|█████     | 5/10 [00:11<00:11,  2.32s/it]\u001b[A\n",
      "validating:  60%|██████    | 6/10 [00:14<00:09,  2.32s/it]\u001b[A\n",
      "validating:  70%|███████   | 7/10 [00:16<00:06,  2.32s/it]\u001b[A\n",
      "validating:  80%|████████  | 8/10 [00:19<00:04,  2.40s/it]\u001b[A\n",
      "validating:  90%|█████████ | 9/10 [00:21<00:02,  2.44s/it]\u001b[A\n",
      "validating: 100%|██████████| 10/10 [00:23<00:00,  2.40s/it]\u001b[A\n",
      "epochs:  40%|████      | 2/5 [03:15<04:53, 97.82s/it]\n",
      "training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "training:   3%|▎         | 1/30 [00:02<01:08,  2.36s/it]\u001b[A\n",
      "training:   7%|▋         | 2/30 [00:04<01:07,  2.39s/it]\u001b[A\n",
      "training:  10%|█         | 3/30 [00:07<01:03,  2.36s/it]\u001b[A\n",
      "training:  13%|█▎        | 4/30 [00:09<01:01,  2.36s/it]\u001b[A\n",
      "training:  17%|█▋        | 5/30 [00:12<01:00,  2.44s/it]\u001b[A\n",
      "training:  20%|██        | 6/30 [00:14<00:58,  2.43s/it]\u001b[A\n",
      "training:  23%|██▎       | 7/30 [00:16<00:55,  2.40s/it]\u001b[A\n",
      "training:  27%|██▋       | 8/30 [00:19<00:52,  2.39s/it]\u001b[A\n",
      "training:  30%|███       | 9/30 [00:21<00:49,  2.38s/it]\u001b[A\n",
      "training:  33%|███▎      | 10/30 [00:23<00:47,  2.36s/it]\u001b[A\n",
      "training:  37%|███▋      | 11/30 [00:26<00:45,  2.40s/it]\u001b[A\n",
      "training:  40%|████      | 12/30 [00:28<00:43,  2.41s/it]\u001b[A\n",
      "training:  43%|████▎     | 13/30 [00:31<00:40,  2.39s/it]\u001b[A\n",
      "training:  47%|████▋     | 14/30 [00:33<00:38,  2.38s/it]\u001b[A\n",
      "training:  50%|█████     | 15/30 [00:35<00:35,  2.39s/it]\u001b[A\n",
      "training:  53%|█████▎    | 16/30 [00:38<00:33,  2.38s/it]\u001b[A\n",
      "training:  57%|█████▋    | 17/30 [00:40<00:31,  2.40s/it]\u001b[A\n",
      "training:  60%|██████    | 18/30 [00:43<00:29,  2.46s/it]\u001b[A\n",
      "training:  63%|██████▎   | 19/30 [00:45<00:26,  2.44s/it]\u001b[A\n",
      "training:  67%|██████▋   | 20/30 [00:47<00:23,  2.40s/it]\u001b[A\n",
      "training:  70%|███████   | 21/30 [00:50<00:21,  2.39s/it]\u001b[A\n",
      "training:  73%|███████▎  | 22/30 [00:52<00:19,  2.40s/it]\u001b[A\n",
      "training:  77%|███████▋  | 23/30 [00:55<00:16,  2.37s/it]\u001b[A\n",
      "training:  80%|████████  | 24/30 [00:57<00:14,  2.37s/it]\u001b[A\n",
      "training:  83%|████████▎ | 25/30 [01:00<00:12,  2.44s/it]\u001b[A\n",
      "training:  87%|████████▋ | 26/30 [01:02<00:09,  2.43s/it]\u001b[A\n",
      "training:  90%|█████████ | 27/30 [01:04<00:07,  2.42s/it]\u001b[A\n",
      "training:  93%|█████████▎| 28/30 [01:07<00:04,  2.39s/it]\u001b[A\n",
      "training:  97%|█████████▋| 29/30 [01:09<00:02,  2.39s/it]\u001b[A\n",
      "training: 100%|██████████| 30/30 [01:11<00:00,  2.40s/it]\u001b[A\n",
      "\n",
      "validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "validating:  10%|█         | 1/10 [00:02<00:21,  2.36s/it]\u001b[A\n",
      "validating:  20%|██        | 2/10 [00:04<00:19,  2.39s/it]\u001b[A\n",
      "validating:  30%|███       | 3/10 [00:07<00:16,  2.35s/it]\u001b[A\n",
      "validating:  40%|████      | 4/10 [00:09<00:13,  2.33s/it]\u001b[A\n",
      "validating:  50%|█████     | 5/10 [00:11<00:11,  2.31s/it]\u001b[A\n",
      "validating:  60%|██████    | 6/10 [00:13<00:09,  2.31s/it]\u001b[A\n",
      "validating:  70%|███████   | 7/10 [00:16<00:06,  2.29s/it]\u001b[A\n",
      "validating:  80%|████████  | 8/10 [00:18<00:04,  2.38s/it]\u001b[A\n",
      "validating:  90%|█████████ | 9/10 [00:21<00:02,  2.36s/it]\u001b[A\n",
      "validating: 100%|██████████| 10/10 [00:23<00:00,  2.34s/it]\u001b[A\n",
      "epochs:  60%|██████    | 3/5 [04:51<03:13, 96.68s/it]\n",
      "training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "training:   3%|▎         | 1/30 [00:02<01:09,  2.39s/it]\u001b[A\n",
      "training:   7%|▋         | 2/30 [00:04<01:06,  2.36s/it]\u001b[A\n",
      "training:  10%|█         | 3/30 [00:07<01:03,  2.34s/it]\u001b[A\n",
      "training:  13%|█▎        | 4/30 [00:09<01:02,  2.39s/it]\u001b[A\n",
      "training:  17%|█▋        | 5/30 [00:12<01:05,  2.62s/it]\u001b[A\n",
      "training:  20%|██        | 6/30 [00:14<01:01,  2.55s/it]\u001b[A\n",
      "training:  23%|██▎       | 7/30 [00:17<00:57,  2.50s/it]\u001b[A\n",
      "training:  27%|██▋       | 8/30 [00:19<00:53,  2.43s/it]\u001b[A\n",
      "training:  30%|███       | 9/30 [00:21<00:50,  2.41s/it]\u001b[A\n",
      "training:  33%|███▎      | 10/30 [00:24<00:48,  2.42s/it]\u001b[A\n",
      "training:  37%|███▋      | 11/30 [00:26<00:45,  2.41s/it]\u001b[A\n",
      "training:  40%|████      | 12/30 [00:29<00:44,  2.45s/it]\u001b[A\n",
      "training:  43%|████▎     | 13/30 [00:31<00:41,  2.42s/it]\u001b[A\n",
      "training:  47%|████▋     | 14/30 [00:34<00:38,  2.40s/it]\u001b[A\n",
      "training:  50%|█████     | 15/30 [00:36<00:35,  2.39s/it]\u001b[A\n",
      "training:  53%|█████▎    | 16/30 [00:38<00:33,  2.38s/it]\u001b[A\n",
      "training:  57%|█████▋    | 17/30 [00:41<00:30,  2.38s/it]\u001b[A\n",
      "training:  60%|██████    | 18/30 [00:43<00:29,  2.45s/it]\u001b[A\n",
      "training:  63%|██████▎   | 19/30 [00:46<00:26,  2.45s/it]\u001b[A\n",
      "training:  67%|██████▋   | 20/30 [00:48<00:24,  2.44s/it]\u001b[A\n",
      "training:  70%|███████   | 21/30 [00:51<00:21,  2.42s/it]\u001b[A\n",
      "training:  73%|███████▎  | 22/30 [00:53<00:19,  2.42s/it]\u001b[A\n",
      "training:  77%|███████▋  | 23/30 [00:55<00:17,  2.45s/it]\u001b[A\n",
      "training:  80%|████████  | 24/30 [00:58<00:14,  2.43s/it]\u001b[A\n",
      "training:  83%|████████▎ | 25/30 [01:00<00:12,  2.48s/it]\u001b[A\n",
      "training:  87%|████████▋ | 26/30 [01:03<00:10,  2.51s/it]\u001b[A\n",
      "training:  90%|█████████ | 27/30 [01:05<00:07,  2.47s/it]\u001b[A\n",
      "training:  93%|█████████▎| 28/30 [01:08<00:04,  2.50s/it]\u001b[A\n",
      "training:  97%|█████████▋| 29/30 [01:10<00:02,  2.47s/it]\u001b[A\n",
      "training: 100%|██████████| 30/30 [01:13<00:00,  2.44s/it]\u001b[A\n",
      "\n",
      "validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "validating:  10%|█         | 1/10 [00:02<00:22,  2.53s/it]\u001b[A\n",
      "validating:  20%|██        | 2/10 [00:04<00:19,  2.49s/it]\u001b[A\n",
      "validating:  30%|███       | 3/10 [00:07<00:16,  2.40s/it]\u001b[A\n",
      "validating:  40%|████      | 4/10 [00:09<00:14,  2.35s/it]\u001b[A\n",
      "validating:  50%|█████     | 5/10 [00:11<00:11,  2.33s/it]\u001b[A\n",
      "validating:  60%|██████    | 6/10 [00:14<00:09,  2.32s/it]\u001b[A\n",
      "validating:  70%|███████   | 7/10 [00:16<00:06,  2.32s/it]\u001b[A\n",
      "validating:  80%|████████  | 8/10 [00:19<00:04,  2.39s/it]\u001b[A\n",
      "validating:  90%|█████████ | 9/10 [00:21<00:02,  2.36s/it]\u001b[A\n",
      "validating: 100%|██████████| 10/10 [00:23<00:00,  2.36s/it]\u001b[A\n",
      "epochs:  80%|████████  | 4/5 [06:28<01:36, 96.79s/it]\n",
      "training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "training:   3%|▎         | 1/30 [00:02<01:08,  2.38s/it]\u001b[A\n",
      "training:   7%|▋         | 2/30 [00:04<01:05,  2.33s/it]\u001b[A\n",
      "training:  10%|█         | 3/30 [00:07<01:03,  2.34s/it]\u001b[A\n",
      "training:  13%|█▎        | 4/30 [00:09<01:01,  2.36s/it]\u001b[A\n",
      "training:  17%|█▋        | 5/30 [00:11<01:00,  2.42s/it]\u001b[A\n",
      "training:  20%|██        | 6/30 [00:14<00:57,  2.40s/it]\u001b[A\n",
      "training:  23%|██▎       | 7/30 [00:16<00:54,  2.38s/it]\u001b[A\n",
      "training:  27%|██▋       | 8/30 [00:18<00:52,  2.37s/it]\u001b[A\n",
      "training:  30%|███       | 9/30 [00:21<00:49,  2.37s/it]\u001b[A\n",
      "training:  33%|███▎      | 10/30 [00:23<00:47,  2.37s/it]\u001b[A\n",
      "training:  37%|███▋      | 11/30 [00:26<00:45,  2.40s/it]\u001b[A\n",
      "training:  40%|████      | 12/30 [00:28<00:43,  2.42s/it]\u001b[A\n",
      "training:  43%|████▎     | 13/30 [00:31<00:41,  2.42s/it]\u001b[A\n",
      "training:  47%|████▋     | 14/30 [00:33<00:38,  2.41s/it]\u001b[A\n",
      "training:  50%|█████     | 15/30 [00:35<00:35,  2.38s/it]\u001b[A\n",
      "training:  53%|█████▎    | 16/30 [00:38<00:33,  2.38s/it]\u001b[A\n",
      "training:  57%|█████▋    | 17/30 [00:40<00:30,  2.36s/it]\u001b[A\n",
      "training:  60%|██████    | 18/30 [00:43<00:29,  2.42s/it]\u001b[A\n",
      "training:  63%|██████▎   | 19/30 [00:45<00:26,  2.41s/it]\u001b[A\n",
      "training:  67%|██████▋   | 20/30 [00:48<00:25,  2.56s/it]\u001b[A\n",
      "training:  70%|███████   | 21/30 [00:50<00:22,  2.54s/it]\u001b[A\n",
      "training:  73%|███████▎  | 22/30 [00:53<00:19,  2.49s/it]\u001b[A\n",
      "training:  77%|███████▋  | 23/30 [00:55<00:17,  2.47s/it]\u001b[A\n",
      "training:  80%|████████  | 24/30 [00:58<00:14,  2.46s/it]\u001b[A\n",
      "training:  83%|████████▎ | 25/30 [01:00<00:12,  2.47s/it]\u001b[A\n",
      "training:  87%|████████▋ | 26/30 [01:02<00:09,  2.43s/it]\u001b[A\n",
      "training:  90%|█████████ | 27/30 [01:05<00:07,  2.40s/it]\u001b[A\n",
      "training:  93%|█████████▎| 28/30 [01:07<00:04,  2.39s/it]\u001b[A\n",
      "training:  97%|█████████▋| 29/30 [01:09<00:02,  2.39s/it]\u001b[A\n",
      "training: 100%|██████████| 30/30 [01:12<00:00,  2.41s/it]\u001b[A\n",
      "\n",
      "validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "validating:  10%|█         | 1/10 [00:02<00:22,  2.51s/it]\u001b[A\n",
      "validating:  20%|██        | 2/10 [00:04<00:19,  2.38s/it]\u001b[A\n",
      "validating:  30%|███       | 3/10 [00:07<00:16,  2.35s/it]\u001b[A\n",
      "validating:  40%|████      | 4/10 [00:09<00:13,  2.33s/it]\u001b[A\n",
      "validating:  50%|█████     | 5/10 [00:11<00:11,  2.31s/it]\u001b[A\n",
      "validating:  60%|██████    | 6/10 [00:14<00:09,  2.33s/it]\u001b[A\n",
      "validating:  70%|███████   | 7/10 [00:16<00:06,  2.33s/it]\u001b[A\n",
      "validating:  80%|████████  | 8/10 [00:18<00:04,  2.39s/it]\u001b[A\n",
      "validating:  90%|█████████ | 9/10 [00:21<00:02,  2.36s/it]\u001b[A\n",
      "validating: 100%|██████████| 10/10 [00:23<00:00,  2.35s/it]\u001b[A\n",
      "epochs: 100%|██████████| 5/5 [08:03<00:00, 96.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epoch_accuracy_list = []\n",
    "\n",
    "for epoch in tqdm(range(5), desc='epochs'):\n",
    "    for x, y in tqdm(train_data_loader, desc='training'):\n",
    "        y_hat = model(x.view(-1, size_of_image))\n",
    "        loss = criterion(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    accuracy = []\n",
    "    for x, y in tqdm(valid_data_loader, desc='validating'):\n",
    "        y_hat = model(x.view(-1, size_of_image))\n",
    "        accuracy.append((torch.argmax(y_hat, dim=1) == y).sum().item() / len(y))\n",
    "    \n",
    "    epoch_accuracy_list.append(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_accuracy_list)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2>\n",
    " <a href=\\\"https://www.linkedin.com/in/joseph-s-50398b136/\\\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2019 <a href=\"cognitiveclass.ai\"> cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
